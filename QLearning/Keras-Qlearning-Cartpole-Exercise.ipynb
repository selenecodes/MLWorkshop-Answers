{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting our program's arguments\n",
    "\n",
    "There are a few arguments we can change that can influence the model, environment or debugging of this notebook.\n",
    "\n",
    "> **Environments**: For all possible environments please have a look at: [OpenAI Environments](https://github.com/openai/gym/wiki/Table-of-environments)\n",
    "> <br>Do note however that I have not tested this behaviour and you'll likely have to tune your hyperparameters accordingly and change some code\n",
    "\n",
    "| Variable | Description | Default | Possible values | Type |\n",
    "| :- | :- | :- | :- | :- |\n",
    "|| **ENVIRONMENT OPTIONS** ||||\n",
    "| **ENVIRONMENT** | The selected environment (from the environments list or just typed in by hand) | 'CartPole-v1' | A string representing a gym environment | str |\n",
    "|| **DEBUG OPTIONS** ||||\n",
    "| **VERBOSE**  | What types of debug data are printed. 0 for no debug printing, 1 for normal debug printing and 2 for normal + GPU device placement printing (this option is very spammy and resource intensive) | 1 | 0/1/2 | int\n",
    "| **VISUALIZE['train']** | Whether to visualize training | False | - | bool |\n",
    "| **VISUALIZE['evaluate']** | Whether to visualize evaluation | True | - | bool |\n",
    "|| **AGENT OPTIONS** ||||\n",
    "| **MODE** | What the program should be doing | 'both' | 'train' / 'evaluate' / 'both' | str |\n",
    "| **SCORE_H5** | If you already have a saved model load it from `./{ENVIRONMENT}-{SCORE_H5}.h5` | None | - | str |\n",
    "| **TRAIN_STEPS** | The amount of training games we wish to play | 500 | > 0 | int |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = 'CartPole-v1'\n",
    "\n",
    "# Parameters\n",
    "MODE = 'both'\n",
    "SCORE_H5 = None # load the .h5 file associated with this score if it exists\n",
    "TRAIN_STEPS = 500\n",
    "\n",
    "# Logging and visualisation\n",
    "VERBOSE = 1\n",
    "VISUALIZE = {\n",
    "    'train': False,\n",
    "    'evaluate': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding our packages\n",
    "\n",
    "If your notebook stops here please make sure you have tensorflow > 2.0 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from keras_helpers import importantText, checkTensorflowSetup\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import os\n",
    "\n",
    "# The functions to create the model with\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Purely for logging\n",
    "from gym.spaces.discrete import Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our environment\n",
    "\n",
    "Our model has inputs and outputs and they are as follows.\n",
    "- Our inputs are the things we perceive also known as the observation space (`env.observation_space`)\n",
    "- Our outputs are the things we can do in this observation_space also known as the actions (`env.action_space`)\n",
    "\n",
    "Now there's two different types of spaces in OpenAI gym.\n",
    "- Discrete: discrete spaces/actions are actions that are either on or off but never inbetween (0/1)\n",
    "- Box/Real: these spaces can be partly on or partly off (0,.1,.11,.5,1)\n",
    "\n",
    "Usually box action spaces tend to have less actions since you have more control over every individual action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "\n",
    "# Check if we're running a Discrete (on/off) or Box action_space (0,.1,.11,1)\n",
    "if (type(env.action_space) == Discrete):\n",
    "    possible_actions = env.action_space.n\n",
    "    ACTION_TYPE = 'discrete (on/off)'\n",
    "else:\n",
    "    possible_actions = env.action_space.shape[0]\n",
    "    ACTION_TYPE = 'real (.1,.5,1)'\n",
    "\n",
    "observation_shape = env.observation_space.shape\n",
    "\n",
    "# Log some information about the environment you're playing\n",
    "print(f'You\\'re playing {ENVIRONMENT}')\n",
    "print(f'You have control over {possible_actions} {ACTION_TYPE} actions')\n",
    "print(f'Your observable space has a shape of: {observation_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking our TensorFlow setup\n",
    "\n",
    "Here we can check if we run on GPU or CPU and what tensorflow version is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkTensorflowSetup(VERBOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your model\n",
    "\n",
    "A model always has a certain amount of input, output and a way to transform this input to output.\n",
    "The little knobs we can turn to change that transformation are called hyperparameters, our environment has the following:\n",
    "\n",
    "### HyperParameters\n",
    "| Math Symbol | Variable | Description | Default | Possible values | Type |\n",
    "| :- | :- | :- | :- | :- | :- |\n",
    "| - | **inputs** | The input size | 2 | > 0 | int |\n",
    "| - | **outputs** | The output size | 4 | > 0 | int |\n",
    "| - | **learning_rate** | The initial step size of our model | 1e-3 | > 0 | float |\n",
    "| - | **memory** | How much our network can remember | 2k | > 0 | int |\n",
    "| Œ≥ | **gamma** | How important an action is to us in the future | 0.95 | 0 - 1 | float |\n",
    "| ùúñ | **epsilon** | The chance of our network taking a random actions instead of a prediction | 1.0 | 0 - 1 | float |\n",
    "| ùúñ | **epsilon_low** | (EPSILON lower bound) How liberal our network is once it's learned patterns | 1e-2 | 0 - <1 | float |\n",
    "| ùúñ-greedy | **epsilon_decay** | How much EPSILON decreases by (`EPSILON *= EPSILON_DECAY`) | 0.95 | 0 - <1 | float |\n",
    "| - | **batch_size** | How much memory it trains on | 32 | > 0 | int |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, observation_shape, number_of_actions, environment_name):\n",
    "        self.inputs = observation_shape\n",
    "        self.outputs = number_of_actions\n",
    "        self.env_name = environment_name # the model doesn't use this, it's just used as a name for the weights file\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = .95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_low = 1e-2\n",
    "        self.epsilon_decay = .95\n",
    "        self.learning_rate = 1e-3\n",
    "        self.batch_size = 32\n",
    "        self.model = self._create_model()\n",
    "    \n",
    "    def _create_model(self):\n",
    "        \"\"\" Creates a keras model\n",
    "        \n",
    "            Returns:\n",
    "                (Sequential): A sequential keras model\n",
    "        \"\"\"\n",
    "        # FIXME: Create a keras sequential model with self.inputs, self.outputs and a number of hidden layers\n",
    "        # NOTE: feel free to use this function `Dense(..., activation = 'relu')`\n",
    "        model = Sequential([\n",
    "            # Add a Dense input layer with the input_shape argument equal to self.inputs\n",
    "            # Add some Dense hidden layers\n",
    "            # Add a Dense output layer with the first argument equal to self.outputs\n",
    "        ])\n",
    "        \n",
    "        # FIXME: Finally compile the model using `model.compile` (this function requires the arguments: loss and optimizer)\n",
    "        # For the loss function please use 'mse'\n",
    "        # For the optimizer please use Adam with the learning_rate as an argument\n",
    "        \n",
    "        if VERBOSE == 1:\n",
    "            model.summary()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def save_weights(self, score):\n",
    "        \"\"\" saves the weights into a .h5 file for later use \"\"\"\n",
    "        self.model.save_weights(f\"./{self.env_name}-{score}.h5\")\n",
    "        \n",
    "    def load_weights(self, score):\n",
    "        \"\"\" loads the weights from a .h5 file \"\"\"\n",
    "        self.model.load_weights(f\"./{self.env_name}-{score}.h5\")\n",
    "        print(importantText('Loaded model'))\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "            Returns:\n",
    "                (int / float):\n",
    "                    An action, either predicted or random based on the epsilon trigger.\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon: # See if we explore (do a random action) or predict\n",
    "            return random.randrange(self.outputs)\n",
    "        \n",
    "        prediction = self.model.predict(state)[0]\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                next_prediction = self.model.predict(next_state)[0]\n",
    "                target = reward + self.gamma * np.amax(next_prediction)\n",
    "\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(\n",
    "                state,\n",
    "                target_f,\n",
    "                verbose=0,\n",
    "                epochs=1\n",
    "            )\n",
    "            \n",
    "        # Slowly base actions more on predictions than on random actions\n",
    "        if self.epsilon > self.epsilon_low:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    observation_shape = observation_shape,\n",
    "    number_of_actions = possible_actions,\n",
    "    environment_name = ENVIRONMENT\n",
    ")\n",
    "\n",
    "# If we have a saved model, load it.\n",
    "if (os.path.isfile(f'./{ENVIRONMENT}-{SCORE_H5}.h5')):\n",
    "    agent.load_weights(SCORE_H5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boiling your PC, also known as \"the training phase\"\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-Cu1mJOh11AU/XAIcUyPK0WI/AAAAAAAANNA/BRlNj0Cbt6EJHNH25D4RhB0e6_sbL1Y8QCLcBGAs/s1600/28056576_10213577221682063_7572084637958860851_n.jpg\" width=\"500px\" align=\"right\"/>\n",
    "\n",
    "In this heading all the previous items come together and we'll use our `Agent` to predict and execute actions on the environment (`env`).\n",
    "\n",
    "OpenAI gives us a few built-in functions we can use:\n",
    "- `step` (Execute an action on the environment)\n",
    "- `reset` (Reset the environment)\n",
    "\n",
    "#### What do these functions return?\n",
    "The `reset` function returns the initial state of the environment<br>\n",
    "The `step` function returns the following values:\n",
    "- next_state: The new state that was created because of your action.\n",
    "- reward: The reward you got for executing that action\n",
    "- done: Whether the agent is done (level finished or a fatal action)\n",
    "- info: (`_`) extra info about the environment (we don't use it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (MODE in ['train', 'both']):\n",
    "    SCORES = []\n",
    "\n",
    "    for step in range(TRAIN_STEPS + 1):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_shape[0]])\n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            if VISUALIZE['train']:\n",
    "                env.render()\n",
    "\n",
    "            # FIXME: Execute an action on the environment by using the `act` function of our `Agent` class\n",
    "            action = \n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, observation_shape[0]])\n",
    "\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            score += 1\n",
    "\n",
    "        print(f\"Episode: {step}/{TRAIN_STEPS} score: {score}\")\n",
    "        SCORES.append(score)\n",
    "\n",
    "        # Save our weights every 100 steps\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            mean_score = np.mean(SCORES[-100:])\n",
    "            print(f\"Mean score of last 100 actions is {mean_score}\")\n",
    "            agent.save_weights(mean_score)\n",
    "\n",
    "        # Retrain on memory\n",
    "        agent.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your model\n",
    "\n",
    "So where's the fancy things like viewing our Agent in the environment?\n",
    "Well that's where the evaluation part comes in. Here we can see our agent floundering about in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (MODE in ['evaluate', 'both']):\n",
    "    SCORES = []\n",
    "    for step in range(101):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, observation_shape[0]])\n",
    "        \n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            if VISUALIZE['evaluate']:\n",
    "                env.render()\n",
    "\n",
    "            # FIXME: Execute an action on the environment by using the `act` function of our `Agent` class\n",
    "            action = \n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, observation_shape[0]])\n",
    "            \n",
    "            state = next_state\n",
    "            score += 1\n",
    "        \n",
    "        SCORES.append(score)\n",
    "    \n",
    "    print(importantText(f\"This environment was {'solved' if np.mean(SCORES) > 195 else 'unsolved'}\"))\n",
    "    print(f\"Score was {np.mean(SCORES)}\")\n",
    "          \n",
    "# Lastly close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References / Sources\n",
    "The sources that were used for creating this workshop can be found below. <br>\n",
    "\n",
    "- Deep Q-Learning with Keras and Gym ¬∑ Keon‚Äôs Blog. (2017, 6 februari). Consulted on 29 februari 2020, from https://keon.github.io/deep-q-learning/\n",
    "- OpenAI. (2019a). Gym: A toolkit for developing and comparing reinforcement learning algorithms. Consulted on 29 februari 2020, from https://gym.openai.com/\n",
    "- OpenAI. (2019b, 18 april). Openai/gym. Consulted on 29 februari 2020, from https://github.com/openai/gym/wiki/Table-of-environments\n",
    "- Zychlinski, S. (2019, 23 februari). Towards Data Science. Consulted on 7 march 2020, from https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e\n",
    "- Blok, S. (2019, 2 july). Selenecodes/IPASS. Consulted on 25 februari 2020, from https://github.com/selenecodes/IPASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
